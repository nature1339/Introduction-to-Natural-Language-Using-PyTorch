<!DOCTYPE html>
<!-- saved from url=(0071)https://nlp2024.tistory.com/manage/newpost/58?type=post&returnURL=ENTRY -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style id="mceDefaultStyles" type="text/css">.mce-content-body div.mce-resizehandle {position: absolute;border: 1px solid black;box-sizing: content-box;background: #FFF;width: 7px;height: 7px;z-index: 10000}.mce-content-body .mce-resizehandle:hover {background: #000}.mce-content-body img[data-mce-selected],.mce-content-body hr[data-mce-selected] {outline: 1px solid black;resize: none}.mce-content-body .mce-clonedresizable {position: absolute;outline: 1px dashed black;opacity: .5;filter: alpha(opacity=50);z-index: 10000}.mce-content-body .mce-resize-helper {background: #555;background: rgba(0,0,0,0.75);border-radius: 3px;border: 1px;color: white;display: none;font-family: sans-serif;font-size: 12px;white-space: nowrap;line-height: 14px;margin: 5px 10px;padding: 5px;position: absolute;z-index: 10001}
.mce-visual-caret {position: absolute;background-color: black;background-color: currentcolor;}.mce-visual-caret-hidden {display: none;}*[data-mce-caret] {position: absolute;left: -1000px;right: auto;top: 0;margin: 0;padding: 0;}
.mce-content-body .mce-offscreen-selection {position: absolute;left: -9999999999px;max-width: 1000000px;}.mce-content-body *[contentEditable=false] {cursor: default;}.mce-content-body *[contentEditable=true] {cursor: text;}
</style><link rel="stylesheet" type="text/css" id="u0" href="./font.css"><link rel="stylesheet" type="text/css" id="u1" href="./editor-content.css"><link rel="stylesheet" type="text/css" id="u2" href="./editor-mobile-content.css"><link rel="stylesheet" type="text/css" id="u3" href="./content.css"><link rel="stylesheet" type="text/css" id="u4" href="./editor-content(1).css"><link rel="stylesheet" data-mce-href="https://t1.daumcdn.net/tistory_admin/lib/highlight.js/styles/atom-one-light.min.css" href="./atom-one-light.min.css"></head><body id="tinymce" class="mce-content-body content useless_p_margin" data-id="editor-tistory" contenteditable="true" spellcheck="false" style="overflow-y: hidden; padding-left: 10px; padding-right: 10px; padding-bottom: 50px;" data-mce-style="overflow-y: hidden; padding-left: 10px; padding-right: 10px; padding-bottom: 50px;" role="textbox" aria-multiline="true" aria-label="글 내용 입력"><p data-ke-size="size16">TF-IDF&nbsp;표현 <br>Term-Frequency-Inverse-Document-Frequency <br><br>TF&nbsp;=&nbsp;Term-Frequency&nbsp;단어의&nbsp;등장횟수 <br>=&nbsp;the,&nbsp;a&nbsp;-&gt;&nbsp;자주등장해도&nbsp;의미가&nbsp;없어서 <br>IDF&nbsp;=&nbsp;Inverse-Document-Frequency <br>그&nbsp;수치값을&nbsp;깎자는&nbsp;의미 <br>여러&nbsp;문서에서&nbsp;공통적으로&nbsp;나오는&nbsp;단어들을 <br>수치를&nbsp;낮추기&nbsp;위해서&nbsp;쓰는것</p><p data-ke-size="size16"><br></p><p data-ke-size="size16">N&nbsp;:&nbsp;전체&nbsp;문서&nbsp;개수 <br>Nw:&nbsp;단어&nbsp;w를&nbsp;포함한&nbsp;문서의&nbsp;개수</p><p data-ke-size="size16"><br></p><pre id="code_1726636849218" class="bash hljs" data-ke-language="bash" data-ke-type="codeblock" contenteditable="false">N_the = 10  
IDF(the) = <span class="hljs-built_in">log</span>(11/11) + 1  
N_apple = 2  
IDF(apple) = <span class="hljs-built_in">log</span>(11/3) + 1</pre><figure data-ke-type="image" data-ke-style="alignLeft" data-ke-mobilestyle="widthOrigin"><img src="./img.png" data-origin-width="380" data-origin-height="83" data-filename="Screenshot_15.png" data-mce-src="https://blog.kakaocdn.net/dn/b4JHAU/btsJDq9BqJI/lF7Ww50D5u1lo5MEnxKN11/img.png" data-is-animation="false"></figure><p data-ke-size="size16"><br></p><pre id="code_1726636886292" class="bash hljs" data-ke-language="bash" data-ke-type="codeblock" contenteditable="false">모든 문서에 등장 (즉,  Nw=N )이면  IDF(w)=0 
반대로 한 문서에만 등장하면  IDF(w)=logN</pre><p data-ke-size="size16"><br></p><p data-ke-size="size16">아래그림&nbsp;</p><p data-ke-size="size16">출처: <a href="https://wikidocs.net/31698" target="_blank" rel="noopener noreferrer" data-mce-href="https://wikidocs.net/31698">https://wikidocs.net/31698</a></p><figure id="og_1726637363454" contenteditable="false" data-ke-type="opengraph" data-ke-align="alignCenter" data-og-type="website" data-og-title="04-04 TF-IDF(Term Frequency-Inverse Document Frequency)" data-og-description="이번에는 DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보겠습니다. TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 많…" data-og-host="wikidocs.net" data-og-source-url="https://wikidocs.net/31698" data-og-url="https://wikidocs.net/31698" data-og-image="https://scrap.kakaocdn.net/dn/ddBDxW/hyW22NBKae/P6lfIaZG82kpIfFyuOEvK0/img.png?width=98&amp;height=130&amp;face=0_0_98_130"><a href="https://wikidocs.net/31698" target="_blank" rel="noopener" data-source-url="https://wikidocs.net/31698" data-mce-href="https://wikidocs.net/31698"><div class="og-image" style="background-image: url(&#39;https://scrap.kakaocdn.net/dn/ddBDxW/hyW22NBKae/P6lfIaZG82kpIfFyuOEvK0/img.png?width=98&amp;height=130&amp;face=0_0_98_130&#39;);" data-mce-style="background-image: url(&#39;https://scrap.kakaocdn.net/dn/ddBDxW/hyW22NBKae/P6lfIaZG82kpIfFyuOEvK0/img.png?width=98&amp;height=130&amp;face=0_0_98_130&#39;);"><br></div><div class="og-text"><p class="og-title" data-ke-size="size16">04-04 TF-IDF(Term Frequency-Inverse Document Frequency)</p><p class="og-desc" data-ke-size="size16">이번에는 DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보겠습니다. TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 많…</p><p class="og-host" data-ke-size="size16">wikidocs.net</p></div></a></figure><p data-ke-size="size16"><br></p><p data-ke-size="size16">아래그림)&nbsp;</p><p data-ke-size="size16">문서에 따른 나온 단어의 빈도수</p><figure data-ke-type="image" data-ke-style="alignLeft" data-ke-mobilestyle="widthOrigin"><img src="./img(1).png" width="478" height="177" data-origin-width="721" data-origin-height="267" data-filename="Screenshot_16.png" data-mce-src="https://blog.kakaocdn.net/dn/1CX5u/btsJDE0GfIi/J7i9OcoC4ZLCeQJaxbnL10/img.png" data-is-animation="false"></figure><p data-ke-size="size16"><br></p><p data-ke-size="size16">아래그림)</p><p data-ke-size="size16">단어가 많이 나올수록 IDF값이 적어짐</p><figure data-ke-type="image" data-ke-style="alignLeft" data-ke-mobilestyle="widthOrigin"><img src="./img(2).png" width="200" height="322" data-origin-width="324" data-origin-height="522" data-filename="Screenshot_17.png" data-mce-src="https://blog.kakaocdn.net/dn/ceUe77/btsJDuKSfon/xNHHB7rQIi9lCUynm3VttK/img.png" data-is-animation="false"></figure><pre id="code_1726637649496" class="bash hljs" data-ke-language="bash" data-ke-type="codeblock" contenteditable="false">텐서

고차원 배열(하지만 1,2차원도 포함)
배열: 나열한것(1x4, 2x4... 고차원배열)

스칼라는 하나의 숫자입니다.
벡터는 숫자의 배열입니다.
행렬은 숫자의 2-D 배열입니다.
텐서는 숫자의 N-D 배열입니다.</pre><p data-ke-size="size16"><br></p><pre id="code_1726638245970" class="bash hljs" data-ke-language="bash" data-ke-type="codeblock" contenteditable="false">행렬 A의 열들 중에서 선형 독립인 열들의 개수(2이면 2차원, 3이면 3차원)
[출처] [기초 선형대수] 행렬에서 Rank (랭크) 란?|작성자 PN</pre><figure data-ke-type="image" data-ke-style="alignLeft" data-ke-mobilestyle="widthOrigin"><img src="./img(3).png" width="503" data-origin-width="959" data-origin-height="808" data-filename="Screenshot_18.png" data-mce-src="https://blog.kakaocdn.net/dn/vNuON/btsJFtpTDmk/gtR2RjPQGCdSl2cJHlkS10/img.png" data-is-animation="false"></figure><p data-ke-size="size16"><br></p><pre id="code_1726641257591" class="bash hljs" data-ke-language="bash" data-ke-type="codeblock" contenteditable="false">x = torch.arange(6).view(2,3)
describe(x)
describe(torch.cat([x, x], dim=0))  <span class="hljs-comment"># concatenate</span>
describe(torch.cat([x, x], dim=1))
describe(torch.stack([x, x]))

&gt;&gt;&gt;
 torch.LongTensor
크기: torch.Size([2, 3])
값: 
tensor([[0, 1, 2],
        [3, 4, 5]])

타입: torch.LongTensor
크기: torch.Size([4, 3])
값: 
tensor([[0, 1, 2],
        [3, 4, 5],
        [0, 1, 2],
        [3, 4, 5]])

타입: torch.LongTensor
크기: torch.Size([2, 6])
값: 
tensor([[0, 1, 2, 0, 1, 2],
        [3, 4, 5, 3, 4, 5]])

타입: torch.LongTensor
크기: torch.Size([2, 2, 3])
값: 
tensor([[[0, 1, 2],
         [3, 4, 5]],

        [[0, 1, 2],
         [3, 4, 5]]])</pre><p data-ke-size="size16"><br></p></body></html>